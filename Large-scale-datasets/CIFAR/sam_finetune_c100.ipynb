{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual update step\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"SAM requires closure, please provide it.\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        # put everything on the same device, in case of model parallelism\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building and loading pre-trained model..\n",
      "Loading checkpoint from './vit-t_cifar100_final_290.pth'\n",
      "Pre-trained model loaded successfully.\n",
      "\n",
      "==> Evaluating loaded model before fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1051568/2295402530.py:301: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.load_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | Loss: 1.2419 | Acc: 64.724% (32362/50000) | Time: 11.57s\n",
      "Test  | Loss: 1.7478 | Acc: 53.700% (5370/10000) | Time: 1.64s\n",
      "--------------------------------------------------\n",
      "==> Starting SAM Fine-tuning...\n",
      "\n",
      "--- SAM Fine-tuning Epoch: 1/10 ---\n",
      "Train | Loss: 1.2518 | Acc: 64.312% (32156/50000) | LR: 0.00100 | Time: 38.82s\n",
      "Test  | Loss: 1.7473 | Acc: 53.820% (5382/10000) | Time: 1.69s\n",
      "\n",
      "--- SAM Fine-tuning Epoch: 2/10 ---\n",
      "Train | Loss: 1.2435 | Acc: 64.676% (32338/50000) | LR: 0.00100 | Time: 38.56s\n",
      "Test  | Loss: 1.7487 | Acc: 53.810% (5381/10000) | Time: 1.63s\n",
      "\n",
      "--- SAM Fine-tuning Epoch: 3/10 ---\n",
      "Train | Loss: 1.2463 | Acc: 64.198% (32099/50000) | LR: 0.00100 | Time: 39.00s\n",
      "Test  | Loss: 1.7473 | Acc: 53.810% (5381/10000) | Time: 1.63s\n",
      "\n",
      "--- SAM Fine-tuning Epoch: 4/10 ---\n",
      "Train | Loss: 1.2472 | Acc: 64.446% (32223/50000) | LR: 0.00100 | Time: 39.03s\n",
      "Test  | Loss: 1.7473 | Acc: 53.650% (5365/10000) | Time: 1.61s\n",
      "\n",
      "--- SAM Fine-tuning Epoch: 5/10 ---\n",
      "Train | Loss: 1.2497 | Acc: 64.550% (32275/50000) | LR: 0.00100 | Time: 38.48s\n",
      "Test  | Loss: 1.7462 | Acc: 53.710% (5371/10000) | Time: 1.65s\n",
      "\n",
      "--- SAM Fine-tuning Epoch: 6/10 ---\n",
      "Train | Loss: 1.2473 | Acc: 64.316% (32158/50000) | LR: 0.00100 | Time: 39.68s\n",
      "Test  | Loss: 1.7476 | Acc: 53.540% (5354/10000) | Time: 1.74s\n",
      "\n",
      "--- SAM Fine-tuning Epoch: 7/10 ---\n",
      "Train | Loss: 1.2446 | Acc: 64.480% (32240/50000) | LR: 0.00100 | Time: 39.20s\n",
      "Test  | Loss: 1.7472 | Acc: 53.720% (5372/10000) | Time: 1.86s\n",
      "\n",
      "--- SAM Fine-tuning Epoch: 8/10 ---\n",
      "Train | Loss: 1.2451 | Acc: 64.526% (32263/50000) | LR: 0.00100 | Time: 39.66s\n",
      "Test  | Loss: 1.7472 | Acc: 53.770% (5377/10000) | Time: 1.65s\n",
      "\n",
      "--- SAM Fine-tuning Epoch: 9/10 ---\n",
      "Train | Loss: 1.2460 | Acc: 64.442% (32221/50000) | LR: 0.00100 | Time: 39.62s\n",
      "Test  | Loss: 1.7456 | Acc: 53.750% (5375/10000) | Time: 1.64s\n",
      "\n",
      "--- SAM Fine-tuning Epoch: 10/10 ---\n",
      "Train | Loss: 1.2464 | Acc: 64.470% (32235/50000) | LR: 0.00100 | Time: 39.32s\n",
      "Test  | Loss: 1.7458 | Acc: 53.710% (5371/10000) | Time: 1.61s\n",
      "\n",
      "==> Finished SAM Fine-tuning in 408.16 seconds.\n",
      "==> Saving final fine-tuned model to ./vit-t_cifar100_sam_ft_290.pth\n",
      "Final fine-tuned model saved.\n",
      "\n",
      "==> Evaluating final fine-tuned model (after 10 epochs)...\n",
      "--- Final Training Set Evaluation ---\n",
      "Train | Loss: 1.2418 | Acc: 64.574% (32287/50000) | Time: 11.57s\n",
      "--- Final Test Set Evaluation ---\n",
      "Test  | Loss: 1.7458 | Acc: 53.710% (5371/10000) | Time: 1.63s\n",
      "\n",
      "===== Initial Model Performance =====\n",
      "Initial Training Loss: 1.2419\n",
      "Initial Training Acc:  64.724%\n",
      "Initial Test Loss:     1.7478\n",
      "Initial Test Acc:      53.700%\n",
      "====================================\n",
      "\n",
      "===== Final Fine-tuned Model Performance =====\n",
      "Final Training Loss: 1.2418\n",
      "Final Training Acc:  64.574%\n",
      "Final Test Loss:     1.7458\n",
      "Final Test Acc:      53.710%\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "\n",
    "# --- SAM Optimizer Definition (Paste the SAM class definition from above here) ---\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "        # Ensure base_optimizer is a type, not an instance\n",
    "        if not isinstance(base_optimizer, type):\n",
    "            raise ValueError(\"base_optimizer must be a class type, e.g., torch.optim.SGD\")\n",
    "        # Instantiate the base_optimizer with the parameters and kwargs\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                # Calculate ascent direction\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                # Restore original weights before base optimizer step\n",
    "                p.data = self.state[p][\"old_p\"]\n",
    "        # The gradients computed in the second forward/backward pass are already set\n",
    "        self.base_optimizer.step()  # do the actual update step using gradients computed on perturbed weights\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    # step() function using closure is not the standard way SAM is used in loops\n",
    "    # It's more common to manually call first_step and second_step in the training loop\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        # Tolenrant to device placement\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device\n",
    "        norms = []\n",
    "        for group in self.param_groups:\n",
    "             for p in group[\"params\"]:\n",
    "                 if p.grad is not None:\n",
    "                     # Use p.grad.detach() to avoid modifying gradients during norm calculation if adaptive=True\n",
    "                     param_grad = p.grad.detach()\n",
    "                     param_norm = ((torch.abs(p.detach()) if group[\"adaptive\"] else 1.0) * param_grad).norm(p=2)\n",
    "                     norms.append(param_norm.to(shared_device))\n",
    "        if not norms: # Handle case where no parameters have gradients\n",
    "            return torch.tensor(0.0, device=shared_device)\n",
    "        # Stack norms before calculating the final norm\n",
    "        total_norm = torch.norm(torch.stack(norms), p=2)\n",
    "        return total_norm\n",
    "\n",
    "    # Overwrite zero_grad to also zero base_optimizer's gradients\n",
    "    def zero_grad(self, set_to_none: bool = False):\n",
    "        super(SAM, self).zero_grad(set_to_none=set_to_none)\n",
    "        self.base_optimizer.zero_grad(set_to_none=set_to_none)\n",
    "\n",
    "    # Need to handle state dict loading/saving properly for both SAM and base_optimizer\n",
    "    def state_dict(self):\n",
    "        # Combine SAM state and base optimizer state\n",
    "        sam_state = super(SAM, self).state_dict()\n",
    "        base_state = self.base_optimizer.state_dict()\n",
    "        return {\"sam_state\": sam_state, \"base_optimizer_state\": base_state}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        # Load states separately\n",
    "        sam_state = state_dict[\"sam_state\"]\n",
    "        base_state = state_dict[\"base_optimizer_state\"]\n",
    "        super(SAM, self).load_state_dict(sam_state)\n",
    "        self.base_optimizer.load_state_dict(base_state)\n",
    "        # Ensure param_groups are synchronized after loading state\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, num=100):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "# -------------------- ResNet20 定义 --------------------\n",
    "# (ResNet20 定义代码保持不变，此处省略以节省空间)\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "        # Don't apply weight init here if loading pretrained weights\n",
    "        # self.apply(_weights_init)\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride_val in strides: # Renamed variable to avoid conflict\n",
    "            layers.append(block(self.in_planes, planes, stride_val)) # Use renamed variable\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # Use adaptive average pooling for robustness\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3], num_classes=100)\n",
    "def ResNet32():\n",
    "    \"\"\" ResNet-32 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (32-2)/6 = 5\n",
    "    return ResNet(BasicBlock, [5, 5, 5], num_classes=100)\n",
    "\n",
    "def ResNet44():\n",
    "    \"\"\" ResNet-44 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (44-2)/6 = 7\n",
    "    return ResNet(BasicBlock, [7, 7, 7], num_classes=100)\n",
    "\n",
    "def ResNet56():\n",
    "    \"\"\" ResNet-56 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (56-2)/6 = 9\n",
    "    return ResNet(BasicBlock, [9, 9, 9], num_classes=100)\n",
    "\n",
    "def ResNet110():\n",
    "    \"\"\" ResNet-110 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (110-2)/6 = 18\n",
    "    return ResNet(BasicBlock, [18, 18, 18], num_classes=100)\n",
    "\n",
    "# Hardcode the values instead:\n",
    "class Args:\n",
    "    model = 'vit-t'\n",
    "    load_path = './vit-t_cifar100_final_290.pth'\n",
    "    ft_epochs = 10\n",
    "    ft_lr = 0.001  # 0.001\n",
    "    sam_rho = 0.01\n",
    "    save_path = './vit-t_cifar100_sam_ft_290.pth'\n",
    "    batch_size = 128\n",
    "    data_path = './data'\n",
    "args = Args()\n",
    "\n",
    "# -------------------- 设备配置 --------------------\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------- 数据准备 --------------------\n",
    "print('==> Preparing data..')\n",
    "cifar100_mean = (0.5071, 0.4867, 0.4408) # CIFAR-100 specific mean\n",
    "cifar100_std = (0.2675, 0.2565, 0.2761)   # CIFAR-100 specific std\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar100_mean, cifar100_std), # <--- MODIFIED to use CIFAR-100 stats\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar100_mean, cifar100_std), # <--- MODIFIED to use CIFAR-100 stats\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100( # <--- MODIFIED\n",
    "    root=args.data_path, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100( # <--- MODIFIED\n",
    "    root=args.data_path, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# -------------------- 模型加载 --------------------\n",
    "print('==> Building and loading pre-trained model..')\n",
    "# net = ResNet20()\n",
    "# net = net.to(device)\n",
    "if args.model == 'r20':\n",
    "    net = ResNet20().to(device)\n",
    "elif args.model == 'r32':\n",
    "    net = ResNet32().to(device)\n",
    "elif args.model == 'r44':\n",
    "    net = ResNet44().to(device)\n",
    "elif args.model == 'r56':\n",
    "    net = ResNet56().to(device)\n",
    "elif args.model == 'r110':\n",
    "    net = ResNet110().to(device)\n",
    "elif args.model == 'vit-t':\n",
    "    net = VisionTransformer(img_size=32,patch_size=4,num_classes=100, embed_dim=192, depth=12, num_heads=3).to(device)\n",
    "elif args.model == 'vit-s':\n",
    "    net = VisionTransformer(img_size=32,patch_size=4,num_classes=100, embed_dim=384, depth=12, num_heads=6).to(device)\n",
    "elif args.model == 'vgg16':\n",
    "    net = VGG('VGG16').to(device)\n",
    "elif args.model == 'vgg11':\n",
    "    net = VGG('VGG11').to(device)\n",
    "\n",
    "if os.path.exists(args.load_path):\n",
    "    try:\n",
    "        print(f\"Loading checkpoint from '{args.load_path}'\")\n",
    "        checkpoint = torch.load(args.load_path, map_location=device)\n",
    "        # Adjust based on how the model was saved (state_dict vs full model)\n",
    "        if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
    "             net.load_state_dict(checkpoint['state_dict'])\n",
    "        elif isinstance(checkpoint, dict) and not ('state_dict' in checkpoint): # Directly saved state_dict\n",
    "             net.load_state_dict(checkpoint)\n",
    "        else: # Saved the entire model object\n",
    "             net = checkpoint\n",
    "        print(\"Pre-trained model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        print(\"Proceeding with initialized ResNet20 (training from scratch).\")\n",
    "        # Apply weight initialization if not loading weights\n",
    "        net.apply(_weights_init)\n",
    "else:\n",
    "    print(f\"Checkpoint file not found at '{args.load_path}'.\")\n",
    "    print(\"Proceeding with initialized ResNet20 (training from scratch).\")\n",
    "    # Apply weight initialization if not loading weights\n",
    "    net.apply(_weights_init)\n",
    "\n",
    "\n",
    "# Optional DataParallel\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "#     net = torch.nn.DataParallel(net)\n",
    "#     cudnn.benchmark = True # Good if input sizes don't change\n",
    "\n",
    "# -------------------- 损失函数 和 SAM 优化器 --------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the base optimizer (SGD) with the fine-tuning learning rate\n",
    "# SAM will use this base optimizer internally\n",
    "base_optimizer = torch.optim.SGD  # Pass the class, not an instance\n",
    "optimizer = SAM(net.parameters(), base_optimizer, rho=args.sam_rho, adaptive=False, # Set adaptive=True if needed\n",
    "                lr=args.ft_lr, momentum=0, weight_decay=5e-4)\n",
    "\n",
    "# No learning rate scheduler needed for fixed LR fine-tuning\n",
    "\n",
    "# -------------------- 评估函数 (Same as before) --------------------\n",
    "def evaluate(loader, set_name=\"Test\", model=net): # Pass model explicitly\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs) # Use the passed model\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    avg_loss = eval_loss / len(loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'{set_name.ljust(5)} | Loss: {avg_loss:.4f} | Acc: {accuracy:.3f}% ({correct}/{total}) | Time: {epoch_time:.2f}s')\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# -------------------- 初始评估 (评估加载的模型) --------------------\n",
    "print(\"\\n==> Evaluating loaded model before fine-tuning...\")\n",
    "initial_train_loss, initial_train_acc = evaluate(trainloader, \"Train\", net)\n",
    "initial_test_loss, initial_test_acc = evaluate(testloader, \"Test\", net)\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "\n",
    "# -------------------- SAM Fine-tuning 训练函数 --------------------\n",
    "def train_sam(epoch):\n",
    "    print(f'\\n--- SAM Fine-tuning Epoch: {epoch+1}/{args.ft_epochs} ---')\n",
    "    net.train() # Set model to training mode\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    current_lr = optimizer.param_groups[0]['lr'] # Get LR (should be fixed)\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # --- SAM specific steps ---\n",
    "        # 1. First forward/backward pass to compute gradients on original weights\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.first_step(zero_grad=True) # Perturbs weights and zeros grads\n",
    "\n",
    "        # 2. Second forward/backward pass on perturbed weights\n",
    "        # Ensure gradients are enabled for the second pass's backward\n",
    "        with torch.enable_grad():\n",
    "             criterion(net(inputs), targets).backward()\n",
    "        optimizer.second_step(zero_grad=True) # Restores original weights and performs update step\n",
    "        # --- End SAM steps ---\n",
    "\n",
    "        # Accumulate loss (using loss from the first step for reporting)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1) # Use predictions from the first step\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    epoch_loss = train_loss / len(trainloader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    print(f'Train | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.3f}% ({correct}/{total}) | LR: {current_lr:.5f} | Time: {epoch_time:.2f}s')\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# -------------------- Fine-tuning 主循环 --------------------\n",
    "print(\"==> Starting SAM Fine-tuning...\")\n",
    "finetuning_start_time = time.time()\n",
    "\n",
    "for epoch in range(args.ft_epochs):\n",
    "    # 1. Fine-tune with SAM for one epoch\n",
    "    train_loss, train_acc = train_sam(epoch)\n",
    "\n",
    "    # 2. Evaluate on the test set after this epoch\n",
    "    test_loss, test_acc = evaluate(testloader, \"Test\", net) # Evaluate the updated model\n",
    "\n",
    "    # No scheduler.step() needed as LR is fixed\n",
    "\n",
    "total_finetuning_time = time.time() - finetuning_start_time\n",
    "print(f\"\\n==> Finished SAM Fine-tuning in {total_finetuning_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# -------------------- 保存最终 Fine-tuned 模型 --------------------\n",
    "print(f'==> Saving final fine-tuned model to {args.save_path}')\n",
    "save_dir = os.path.dirname(args.save_path)\n",
    "if save_dir and not os.path.exists(save_dir): # Check if save_dir is not empty\n",
    "    os.makedirs(save_dir)\n",
    "# Save only the model state_dict is usually preferred\n",
    "torch.save(net.state_dict(), args.save_path)\n",
    "# If you need to save optimizer state as well (e.g., to resume SAM training):\n",
    "# torch.save({\n",
    "#     'epoch': args.ft_epochs, # Or the actual last epoch number\n",
    "#     'model_state_dict': net.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(), # Save SAM state\n",
    "# }, args.save_path)\n",
    "print(\"Final fine-tuned model saved.\")\n",
    "\n",
    "# -------------------- 评估最终 Fine-tuned 模型 --------------------\n",
    "print(\"\\n==> Evaluating final fine-tuned model (after {} epochs)...\".format(args.ft_epochs))\n",
    "print(\"--- Final Training Set Evaluation ---\")\n",
    "final_train_loss, final_train_acc = evaluate(trainloader, \"Train\", net)\n",
    "print(\"--- Final Test Set Evaluation ---\")\n",
    "final_test_loss, final_test_acc = evaluate(testloader, \"Test\", net)\n",
    "\n",
    "print(\"\\n===== Initial Model Performance =====\")\n",
    "print(f\"Initial Training Loss: {initial_train_loss:.4f}\")\n",
    "print(f\"Initial Training Acc:  {initial_train_acc:.3f}%\")\n",
    "print(f\"Initial Test Loss:     {initial_test_loss:.4f}\")\n",
    "print(f\"Initial Test Acc:      {initial_test_acc:.3f}%\")\n",
    "print(\"====================================\")\n",
    "\n",
    "print(\"\\n===== Final Fine-tuned Model Performance =====\")\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final Training Acc:  {final_train_acc:.3f}%\")\n",
    "print(f\"Final Test Loss:     {final_test_loss:.4f}\")\n",
    "print(f\"Final Test Acc:      {final_test_acc:.3f}%\")\n",
    "print(\"==========================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sulw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
