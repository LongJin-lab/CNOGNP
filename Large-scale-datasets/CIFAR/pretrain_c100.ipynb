{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "import torchvision.models as models\n",
    "from models import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Union, List, Dict, Any, cast\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, num=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# -------------------- ResNet20 定义 --------------------\n",
    "# (ResNet20 定义代码保持不变，此处省略以节省空间)\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "        self.apply(_weights_init)\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride_val in strides: # Renamed variable to avoid conflict\n",
    "            layers.append(block(self.in_planes, planes, stride_val)) # Use renamed variable\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # Use adaptive average pooling for robustness to final feature map size\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3], num_classes=100)\n",
    "\n",
    "def ResNet32():\n",
    "    return ResNet(BasicBlock, [5, 5, 5], num_classes=100)\n",
    "\n",
    "def ResNet44():\n",
    "    \"\"\" ResNet-44 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (44-2)/6 = 7\n",
    "    return ResNet(BasicBlock, [7, 7, 7], num_classes=100)\n",
    "\n",
    "def ResNet56():\n",
    "    \"\"\" ResNet-56 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (56-2)/6 = 9\n",
    "    return ResNet(BasicBlock, [9, 9, 9], num_classes=100)\n",
    "\n",
    "def ResNet110():\n",
    "    \"\"\" ResNet-110 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (110-2)/6 = 18\n",
    "    return ResNet(BasicBlock, [18, 18, 18], num_classes=100)\n",
    "\n",
    "# -------------------- 参数设置 --------------------\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR-100 Training')\n",
    "parser.add_argument('--lr', default=0.1, type=float, help='initial learning rate')\n",
    "parser.add_argument('--epochs', default=290, type=int, help='number of epochs to train')  #150 c10 resnet20 190 c100 290 vit \n",
    "parser.add_argument('--batch_size', default=128, type=int, help='batch size')\n",
    "parser.add_argument('--save_path', default='./vit-t_cifar100_final_290.pth', type=str, help='path to save final model')\n",
    "parser.add_argument('--data_path', default='./data', type=str, help='path to dataset')\n",
    "# --- Use parse_known_args() for Jupyter compatibility ---\n",
    "args, unknown = parser.parse_known_args()\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# -------------------- 设备配置 --------------------\n",
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------- 数据准备 --------------------\n",
    "print('==> Preparing data..')\n",
    "cifar100_mean = (0.5071, 0.4867, 0.4408) # CIFAR-100 specific mean\n",
    "cifar100_std = (0.2675, 0.2565, 0.2761)   # CIFAR-100 specific std\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar100_mean, cifar100_std), # <--- MODIFIED to use CIFAR-100 stats\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar100_mean, cifar100_std), # <--- MODIFIED to use CIFAR-100 stats\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100( # <--- MODIFIED\n",
    "    root=args.data_path, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100( # <--- MODIFIED\n",
    "    root=args.data_path, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# -------------------- 模型、损失函数、优化器、学习率调度器 --------------------\n",
    "print('==> Building model..')\n",
    "# net = ResNet110()\n",
    "# net = ResNet56()\n",
    "# vit-t\n",
    "net = VisionTransformer(img_size=32, patch_size=4, num_classes=100, embed_dim=192, depth=12, num_heads=3)\n",
    "# vit-s\n",
    "# net = VisionTransformer(img_size=32,patch_size=4,num_classes=100, embed_dim=384, depth=12, num_heads=6)\n",
    "# net = VGG('VGG11')\n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Optional DataParallel\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "#     net = torch.nn.DataParallel(net)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=0)\n",
    "\n",
    "# -------------------- 训练函数 --------------------\n",
    "def train(epoch):\n",
    "    print(f'\\n--- Epoch: {epoch+1}/{args.epochs} ---')\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    current_lr = optimizer.param_groups[0]['lr'] # Get LR at the start of epoch\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    epoch_loss = train_loss / len(trainloader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    print(f'Train | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.3f}% ({correct}/{total}) | LR: {current_lr:.5f} | Time: {epoch_time:.2f}s')\n",
    "    # Return values if needed elsewhere, though not strictly necessary for printing\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# -------------------- 评估函数 --------------------\n",
    "def evaluate(loader, set_name=\"Test\"):\n",
    "    net.eval()\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    avg_loss = eval_loss / len(loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'{set_name.ljust(5)} | Loss: {avg_loss:.4f} | Acc: {accuracy:.3f}% ({correct}/{total}) | Time: {epoch_time:.2f}s')\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# -------------------- 训练主循环 --------------------\n",
    "print(\"==> Starting Training...\")\n",
    "training_start_time = time.time() # Use a different variable name\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    # 1. Train for one epoch and print training metrics\n",
    "    train_loss, train_acc = train(epoch)\n",
    "\n",
    "    # 2. Evaluate on the test set after this epoch and print test metrics\n",
    "    test_loss, test_acc = evaluate(testloader, \"Test\") # <-- Evaluate on test set\n",
    "\n",
    "    # 3. Update the learning rate for the *next* epoch\n",
    "    scheduler.step()\n",
    "\n",
    "total_training_time = time.time() - training_start_time\n",
    "print(f\"\\n==> Finished Training in {total_training_time:.2f} seconds ({total_training_time/3600:.2f} hours).\")\n",
    "\n",
    "\n",
    "# -------------------- 保存最终模型 --------------------\n",
    "print(f'==> Saving final model to {args.save_path}')\n",
    "save_dir = os.path.dirname(args.save_path)\n",
    "if save_dir and not os.path.exists(save_dir): # Check if save_dir is not empty\n",
    "    os.makedirs(save_dir)\n",
    "torch.save(net.state_dict(), args.save_path)\n",
    "print(\"Final model saved.\")\n",
    "\n",
    "# -------------------- 评估最终模型 --------------------\n",
    "print(\"\\n==> Evaluating final model (performance after {} epochs)...\".format(args.epochs))\n",
    "print(\"--- Final Training Set Evaluation ---\")\n",
    "# Re-evaluate on training set for final numbers (optional, could use last epoch's train results)\n",
    "final_train_loss, final_train_acc = evaluate(trainloader, \"Train\")\n",
    "print(\"--- Final Test Set Evaluation ---\")\n",
    "# Re-evaluate on test set for final numbers (optional, could use last epoch's test results)\n",
    "final_test_loss, final_test_acc = evaluate(testloader, \"Test\")\n",
    "\n",
    "\n",
    "print(\"\\n===== Final Model Performance =====\")\n",
    "print(f\"Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Training Acc:  {final_train_acc:.3f}%\")\n",
    "print(f\"Test Loss:     {final_test_loss:.4f}\")\n",
    "print(f\"Test Acc:      {final_test_acc:.3f}%\")\n",
    "print(\"=================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sulw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
