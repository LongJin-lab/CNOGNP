{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ps/anaconda3/envs/qc/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Loading pre-trained model...\n",
      "Loaded pre-trained weights from './resnet110_cifar10_final.pth'\n",
      "\n",
      "==> Evaluating loaded pre-trained model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1118341/2206825562.py:333: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.load_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  Eval | Loss: 0.2423 | Acc: 94.020% (9402/10000)\n",
      "Train Eval | Loss: 0.0025 | Acc: 99.988% (49994/50000)\n",
      "\n",
      "==> Initializing 2 CNOGNP particles...\n",
      "  Initializing particle 1/2...\n",
      "  Initializing particle 2/2...\n",
      "add noise\n",
      "\n",
      "==> Performing initial fitness evaluation (using CNOGNP fitness)...\n",
      "  Evaluating initial fitness for particle 1/2:\n",
      "Done. Fitness: 1098.9000 (Avg Loss: 999.0000, Avg Grad Norm: 999.0000)\n",
      "  Evaluating initial fitness for particle 2/2:\n",
      "Done. Fitness: 1098.9000 (Avg Loss: 999.0000, Avg Grad Norm: 999.0000)\n",
      "\n",
      "==> Starting CNOGNP Fine-tuning for 10 epochs...\n",
      "\n",
      "--- CNOGNP Epoch 1/10 ---\n",
      "  Processing Particle 1/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0040 | Time: 36.94s\n",
      "Done. Fitness: 0.0452 (Avg Loss: 0.0040, Avg Grad Norm: 0.4128)\n",
      "      New pbest for particle 1: 0.0452 (was 1098.9000)\n",
      "  Updating gbest...\n",
      "    New Global Best! Fitness: 0.0452 (was inf) from particle 1's pbest\n",
      "  Processing Particle 2/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0042 | Time: 35.83s\n",
      "Done. Fitness: 0.0480 (Avg Loss: 0.0042, Avg Grad Norm: 0.4385)\n",
      "      New pbest for particle 2: 0.0480 (was 1098.9000)\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0452, Current gbest: 0.0452\n",
      "--- CNOGNP Epoch 1 finished. Time: 74.24s ---\n",
      "\n",
      "--- CNOGNP Epoch 2/10 ---\n",
      "  Processing Particle 1/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0042 | Time: 36.26s\n",
      "Done. Fitness: 0.0468 (Avg Loss: 0.0042, Avg Grad Norm: 0.4261)\n",
      "      Fitness 0.0468 not better than pbest 0.0452\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0468, Current gbest: 0.0452\n",
      "  Processing Particle 2/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0043 | Time: 36.21s\n",
      "Done. Fitness: 0.0506 (Avg Loss: 0.0043, Avg Grad Norm: 0.4630)\n",
      "      Fitness 0.0506 not better than pbest 0.0480\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0468, Current gbest: 0.0452\n",
      "--- CNOGNP Epoch 2 finished. Time: 73.45s ---\n",
      "\n",
      "--- CNOGNP Epoch 3/10 ---\n",
      "  Processing Particle 1/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0041 | Time: 36.41s\n",
      "Done. Fitness: 0.0461 (Avg Loss: 0.0041, Avg Grad Norm: 0.4197)\n",
      "      Fitness 0.0461 not better than pbest 0.0452\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0461, Current gbest: 0.0452\n",
      "  Processing Particle 2/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0042 | Time: 36.63s\n",
      "Done. Fitness: 0.0467 (Avg Loss: 0.0042, Avg Grad Norm: 0.4252)\n",
      "      New pbest for particle 2: 0.0467 (was 0.0480)\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0461, Current gbest: 0.0452\n",
      "--- CNOGNP Epoch 3 finished. Time: 74.22s ---\n",
      "\n",
      "--- CNOGNP Epoch 4/10 ---\n",
      "  Processing Particle 1/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0041 | Time: 35.97s\n",
      "Done. Fitness: 0.0450 (Avg Loss: 0.0041, Avg Grad Norm: 0.4086)\n",
      "      New pbest for particle 1: 0.0450 (was 0.0452)\n",
      "  Updating gbest...\n",
      "    New Global Best! Fitness: 0.0450 (was 0.0452) from particle 1's pbest\n",
      "  Processing Particle 2/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0040 | Time: 36.47s\n",
      "Done. Fitness: 0.0453 (Avg Loss: 0.0040, Avg Grad Norm: 0.4125)\n",
      "      New pbest for particle 2: 0.0453 (was 0.0467)\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0450, Current gbest: 0.0450\n",
      "--- CNOGNP Epoch 4 finished. Time: 73.86s ---\n",
      "\n",
      "--- CNOGNP Epoch 5/10 ---\n",
      "  Processing Particle 1/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0044 | Time: 35.59s\n",
      "Done. Fitness: 0.0493 (Avg Loss: 0.0044, Avg Grad Norm: 0.4486)\n",
      "      Fitness 0.0493 not better than pbest 0.0450\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0493, Current gbest: 0.0450\n",
      "  Processing Particle 2/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0043 | Time: 35.95s\n",
      "Done. Fitness: 0.0496 (Avg Loss: 0.0043, Avg Grad Norm: 0.4530)\n",
      "      Fitness 0.0496 not better than pbest 0.0453\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0493, Current gbest: 0.0450\n",
      "--- CNOGNP Epoch 5 finished. Time: 72.49s ---\n",
      "\n",
      "--- CNOGNP Epoch 6/10 ---\n",
      "  Processing Particle 1/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0041 | Time: 36.53s\n",
      "Done. Fitness: 0.0458 (Avg Loss: 0.0041, Avg Grad Norm: 0.4168)\n",
      "      Fitness 0.0458 not better than pbest 0.0450\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0458, Current gbest: 0.0450\n",
      "  Processing Particle 2/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0042 | Time: 37.33s\n",
      "Done. Fitness: 0.0477 (Avg Loss: 0.0042, Avg Grad Norm: 0.4349)\n",
      "      Fitness 0.0477 not better than pbest 0.0453\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0458, Current gbest: 0.0450\n",
      "--- CNOGNP Epoch 6 finished. Time: 74.82s ---\n",
      "\n",
      "--- CNOGNP Epoch 7/10 ---\n",
      "  Processing Particle 1/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0041 | Time: 34.66s\n",
      "Done. Fitness: 0.0453 (Avg Loss: 0.0041, Avg Grad Norm: 0.4126)\n",
      "      Fitness 0.0453 not better than pbest 0.0450\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0453, Current gbest: 0.0450\n",
      "  Processing Particle 2/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0042 | Time: 35.86s\n",
      "Done. Fitness: 0.0481 (Avg Loss: 0.0042, Avg Grad Norm: 0.4382)\n",
      "      Fitness 0.0481 not better than pbest 0.0453\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0453, Current gbest: 0.0450\n",
      "--- CNOGNP Epoch 7 finished. Time: 71.64s ---\n",
      "\n",
      "--- CNOGNP Epoch 8/10 ---\n",
      "  Processing Particle 1/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0041 | Time: 34.63s\n",
      "Done. Fitness: 0.0462 (Avg Loss: 0.0041, Avg Grad Norm: 0.4214)\n",
      "      Fitness 0.0462 not better than pbest 0.0450\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0462, Current gbest: 0.0450\n",
      "  Processing Particle 2/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0041 | Time: 35.09s\n",
      "Done. Fitness: 0.0460 (Avg Loss: 0.0041, Avg Grad Norm: 0.4186)\n",
      "      Fitness 0.0460 not better than pbest 0.0453\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0460, Current gbest: 0.0450\n",
      "--- CNOGNP Epoch 8 finished. Time: 70.70s ---\n",
      "\n",
      "--- CNOGNP Epoch 9/10 ---\n",
      "  Processing Particle 1/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0042 | Time: 35.82s\n",
      "Done. Fitness: 0.0481 (Avg Loss: 0.0042, Avg Grad Norm: 0.4387)\n",
      "      Fitness 0.0481 not better than pbest 0.0450\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0481, Current gbest: 0.0450\n",
      "  Processing Particle 2/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0044 | Time: 35.49s\n",
      "Done. Fitness: 0.0502 (Avg Loss: 0.0044, Avg Grad Norm: 0.4586)\n",
      "      Fitness 0.0502 not better than pbest 0.0453\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0481, Current gbest: 0.0450\n",
      "--- CNOGNP Epoch 9 finished. Time: 72.42s ---\n",
      "\n",
      "--- CNOGNP Epoch 10/10 ---\n",
      "  Processing Particle 1/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0042 | Time: 35.62s\n",
      "Done. Fitness: 0.0488 (Avg Loss: 0.0042, Avg Grad Norm: 0.4459)\n",
      "      Fitness 0.0488 not better than pbest 0.0450\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0488, Current gbest: 0.0450\n",
      "  Processing Particle 2/2:\n",
      "      Starting 1-epoch SGD (CNOGNP Line 7, η=0.001)... Done. Avg Loss during SGD: 0.0043 | Time: 37.20s\n",
      "Done. Fitness: 0.0485 (Avg Loss: 0.0043, Avg Grad Norm: 0.4422)\n",
      "      Fitness 0.0485 not better than pbest 0.0453\n",
      "  Updating gbest...\n",
      "    No new gbest found this epoch. Best this epoch: 0.0485, Current gbest: 0.0450\n",
      "--- CNOGNP Epoch 10 finished. Time: 73.83s ---\n",
      "\n",
      "==> Finished CNOGNP Fine-tuning in 731.69 seconds (0.20 hours).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "import copy # For deep copying models and states\n",
    "import math # For infinity\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, num=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# --- ResNet20 Definition (Same as before, omitted for brevity) ---\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd): super().__init__(); self.lambd = lambd\n",
    "    def forward(self, x): return self.lambd(x)\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A': self.shortcut = LambdaLayer(lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B': self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(self.expansion * planes))\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out)); out += self.shortcut(x); out = F.relu(out)\n",
    "        return out\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1); layers = []\n",
    "        for stride_val in strides: layers.append(block(self.in_planes, planes, stride_val)); self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x))); out = self.layer1(out); out = self.layer2(out); out = self.layer3(out)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1)); out = out.view(out.size(0), -1); out = self.linear(out)\n",
    "        return out\n",
    "def ResNet20(): return ResNet(BasicBlock, [3, 3, 3], num_classes=10)\n",
    "\n",
    "def ResNet32():\n",
    "    \"\"\" ResNet-32 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (32-2)/6 = 5\n",
    "    return ResNet(BasicBlock, [5, 5, 5], num_classes=10)\n",
    "\n",
    "def ResNet44():\n",
    "    \"\"\" ResNet-44 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (44-2)/6 = 7\n",
    "    return ResNet(BasicBlock, [7, 7, 7], num_classes=10)\n",
    "\n",
    "def ResNet56():\n",
    "    \"\"\" ResNet-56 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (56-2)/6 = 9\n",
    "    return ResNet(BasicBlock, [9, 9, 9], num_classes=10)\n",
    "\n",
    "def ResNet110():\n",
    "    \"\"\" ResNet-110 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (110-2)/6 = 18\n",
    "    return ResNet(BasicBlock, [18, 18, 18], num_classes=10)\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- CNOGNP & Script Parameters --------------------\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR-10 Fine-tuning with CNOGNP on Weights')\n",
    "# --- CNOGNP Specific ---\n",
    "parser.add_argument('--lambda_gnp', default=0.1, type=float, help='Gradient norm penalty coefficient (λ)')\n",
    "# --- Parameters from CNO (Mapping from image) ---\n",
    "parser.add_argument('--num_particles', default=2, type=int, help='Number of particles (N)')\n",
    "parser.add_argument('--cnognp_epochs', default=10, type=int, help='Number of CNOGNP iterations (κ_max)') # Renamed epoch arg\n",
    "parser.add_argument('--w', default=1, type=float, help='Inertia weight (ω)')\n",
    "parser.add_argument('--c1', default=0.00001, type=float, help='Cognitive learning factor (c1)')\n",
    "parser.add_argument('--c2', default=0.00001, type=float, help='Social learning factor (c2)')\n",
    "parser.add_argument('--eta', default=0.001, type=float, help='Scale factor / Learning rate for inner SGD step (η)')\n",
    "# --- Parameters consistent with others ---\n",
    "parser.add_argument('--initial_noise_level', default=0.0001, type=float, help='Std deviation of noise added to initial particle weights')\n",
    "parser.add_argument('--inner_sgd_momentum', default=0, type=float, help='Momentum for the inner SGD step')\n",
    "parser.add_argument('--inner_sgd_wd', default=5e-4, type=float, help='Weight decay for the inner SGD step')\n",
    "parser.add_argument('--model', default='r110', type=str)\n",
    "parser.add_argument('--load_path', default='./resnet110_cifar10_final.pth', type=str, help='Path to load the pre-trained model')\n",
    "parser.add_argument('--save_path', default='./resnet110_cifar10_cnognp_ft.pth', type=str, help='Path to save the best model found by CNOGNP')\n",
    "parser.add_argument('--batch_size', default=128, type=int, help='Batch size for SGD training and evaluation')\n",
    "parser.add_argument('--data_path', default='./data', type=str, help='Path to dataset')\n",
    "# --- Use parse_known_args() for Jupyter compatibility ---\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- Device Configuration --------------------\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# -------------------- 数据准备 --------------------\n",
    "print('==> Preparing data..')\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=args.data_path, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=args.data_path, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# -------------------- Loss Function --------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# -------------------- Helper Functions --------------------\n",
    "\n",
    "# Standard evaluation function (for final results)\n",
    "def evaluate(loader, model, set_name=\"Test\"):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    avg_loss = eval_loss / len(loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'{set_name.ljust(5)} Eval | Loss: {avg_loss:.4f} | Acc: {accuracy:.3f}% ({correct}/{total})')\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Function for CNO Line 7: Train ONE SGD epoch and return the *new state* and the loss\n",
    "def train_one_sgd_epoch_and_get_state(initial_state_dict, train_loader, criterion, device, lr, momentum, weight_decay, model_name):\n",
    "    if args.model == 'r20':\n",
    "        model = ResNet20().to(device)\n",
    "    elif args.model == 'r32':\n",
    "        model = ResNet32().to(device)\n",
    "    elif args.model == 'r44':\n",
    "        model = ResNet44().to(device)\n",
    "    elif args.model == 'r56':\n",
    "        model = ResNet56().to(device)\n",
    "    elif args.model == 'r110':\n",
    "        model = ResNet110().to(device)\n",
    "    elif args.model == 'vit-t':\n",
    "        model = VisionTransformer(img_size=32,patch_size=4,num_classes=100, embed_dim=192, depth=12, num_heads=3).to(device)\n",
    "    elif args.model == 'vit-s':\n",
    "        model = VisionTransformer(img_size=32,patch_size=4,num_classes=100, embed_dim=384, depth=12, num_heads=6).to(device)\n",
    "    elif args.model == 'vgg16':\n",
    "        model = VGG('VGG16').to(device)\n",
    "    elif args.model == 'vgg11':\n",
    "        model = VGG('VGG11').to(device)\n",
    "    model.load_state_dict(copy.deepcopy(initial_state_dict))\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_grad_norm = 0 # Accumulate norm per batch\n",
    "    num_batches = 0\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    start_time = time.time()\n",
    "    print(f\"      Starting 1-epoch SGD (CNOGNP Line 7, η={lr})... \", end=\"\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        batch_grad_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                batch_grad_norm += param_norm.item() ** 2\n",
    "        batch_grad_norm = batch_grad_norm ** 0.5\n",
    "        total_grad_norm += batch_grad_norm\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    avg_grad_norm = total_grad_norm / len(train_loader) # Average the norm calculated per batch\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Done. Avg Loss during SGD: {avg_loss:.4f} | Time: {epoch_time:.2f}s\")\n",
    "    return copy.deepcopy(model.state_dict()), avg_loss, avg_grad_norm\n",
    "\n",
    "# Function for CNOGNP Line 10: Evaluate fitness (Loss + λ*||g||₂) on train set\n",
    "def evaluate_fitness_loss_and_grad_norm(avg_loss, avg_grad_norm, lambda_gnp):\n",
    "    # CNOGNP Fitness\n",
    "    fitness = avg_loss + lambda_gnp * avg_grad_norm\n",
    "    print(f\"Done. Fitness: {fitness:.4f} (Avg Loss: {avg_loss:.4f}, Avg Grad Norm: {avg_grad_norm:.4f})\")\n",
    "    return fitness\n",
    "\n",
    "# --- Other helper functions (add_noise, initialize_velocity, update_cno_velocity, update_particle_position) ---\n",
    "# --- remain EXACTLY the same as in the CNO implementation ---\n",
    "def add_noise_to_model(model, noise_level, device):\n",
    "    print('add noise')\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters(): param.add_(torch.randn_like(param) * noise_level)\n",
    "    return model\n",
    "def initialize_velocity(model):\n",
    "    velocity = {}\n",
    "    with torch.no_grad():\n",
    "      for name, param in model.named_parameters():\n",
    "          if param.requires_grad: velocity[name] = torch.zeros_like(param)\n",
    "    return velocity\n",
    "def update_cno_velocity(velocity_dict, z_bar_i_state, pbest_state, gbest_state, c1, c2, device):\n",
    "    with torch.no_grad():\n",
    "        for name, param_vel in velocity_dict.items():\n",
    "            if name not in z_i_state: continue\n",
    "            r1 = random.random()\n",
    "            r2 = random.random()\n",
    "\n",
    "            # Ensure all tensors are on the correct device\n",
    "            z_bar_i_param = z_bar_i_state[name].to(device)\n",
    "            pbest_param = pbest_state[name].to(device)\n",
    "            gbest_param = gbest_state[name].to(device)\n",
    "            current_vel = param_vel.to(device)\n",
    "\n",
    "            # CNO Velocity Update (Line 8)\n",
    "            cognitive_term = c1 * r1 * (pbest_param - z_bar_i_param)\n",
    "            social_term = c2 * r2 * (gbest_param - z_bar_i_param)\n",
    "\n",
    "            new_vel = cognitive_term + social_term\n",
    "            velocity_dict[name].copy_(new_vel) # Update velocity in place\n",
    "def update_particle_position(model_to_update, z_bar_i_state, velocity_dict, device):\n",
    "    new_state = copy.deepcopy(z_bar_i_state)\n",
    "    with torch.no_grad():\n",
    "        for name, param in new_state.items():\n",
    "             if name in velocity_dict: param.add_(velocity_dict[name].to(device))\n",
    "    model_to_update.load_state_dict(new_state)\n",
    "\n",
    "# -------------------- Load Pre-trained Model --------------------\n",
    "print('==> Loading pre-trained model...')\n",
    "# initial_model = ResNet20().to(device)\n",
    "if args.model == 'r20':\n",
    "    initial_model = ResNet20().to(device)\n",
    "elif args.model == 'r32':\n",
    "    initial_model = ResNet32().to(device)\n",
    "elif args.model == 'r44':\n",
    "    initial_model = ResNet44().to(device)\n",
    "elif args.model == 'r56':\n",
    "    initial_model = ResNet56().to(device)\n",
    "elif args.model == 'r110':\n",
    "    initial_model = ResNet110().to(device)\n",
    "elif args.model == 'vit-t':\n",
    "    initial_model = VisionTransformer(img_size=32,patch_size=4,num_classes=100, embed_dim=192, depth=12, num_heads=3).to(device)\n",
    "elif args.model == 'vit-s':\n",
    "    initial_model = VisionTransformer(img_size=32,patch_size=4,num_classes=100, embed_dim=384, depth=12, num_heads=6).to(device)\n",
    "elif args.model == 'vgg16':\n",
    "    initial_model = VGG('VGG16').to(device)\n",
    "elif args.model == 'vgg11':\n",
    "    initial_model = VGG('VGG11').to(device)\n",
    "\n",
    "# (Loading logic remains the same as CNO)\n",
    "if os.path.exists(args.load_path):\n",
    "    try:\n",
    "        checkpoint = torch.load(args.load_path, map_location=device)\n",
    "        if isinstance(checkpoint, dict) and 'state_dict' in checkpoint: initial_model.load_state_dict(checkpoint['state_dict'])\n",
    "        elif isinstance(checkpoint, dict): initial_model.load_state_dict(checkpoint)\n",
    "        else: initial_model = checkpoint\n",
    "        print(f\"Loaded pre-trained weights from '{args.load_path}'\")\n",
    "    except Exception as e: print(f\"Error loading checkpoint: {e}. Exiting.\"); exit()\n",
    "else: print(f\"Pre-trained model file not found at '{args.load_path}'. Exiting.\"); exit()\n",
    "\n",
    "# Evaluate the loaded model once (using standard evaluate)\n",
    "print(\"\\n==> Evaluating loaded pre-trained model:\")\n",
    "initial_test_loss, initial_test_acc = evaluate(testloader, initial_model, \"Test\")\n",
    "initial_train_loss, initial_train_acc = evaluate(trainloader, initial_model, \"Train\")\n",
    "\n",
    "\n",
    "# -------------------- CNOGNP Initialization --------------------\n",
    "print(f\"\\n==> Initializing {args.num_particles} CNOGNP particles...\")\n",
    "particles = []\n",
    "gbest_state_dict = None\n",
    "gbest_fitness = float('inf') # Use math.inf for clarity\n",
    "\n",
    "for i in range(args.num_particles):\n",
    "    print(f\"  Initializing particle {i+1}/{args.num_particles}...\")\n",
    "    particle_model = copy.deepcopy(initial_model).to(device)\n",
    "    if i > 0 or args.num_particles == 1:\n",
    "         particle_model = add_noise_to_model(particle_model, args.initial_noise_level, device)\n",
    "    velocity = initialize_velocity(particle_model)\n",
    "    pbest_state_dict = copy.deepcopy(particle_model.state_dict())\n",
    "    pbest_fitness = float('inf')\n",
    "    particles.append({\n",
    "        'id': i, 'model': particle_model, 'velocity': velocity,\n",
    "        'pbest_state_dict': pbest_state_dict, 'pbest_fitness': pbest_fitness,\n",
    "        'current_fitness': float('inf')\n",
    "    })\n",
    "\n",
    "# --- Initial Fitness Evaluation (using CNOGNP fitness function) ---\n",
    "print(\"\\n==> Performing initial fitness evaluation (using CNOGNP fitness)...\")\n",
    "current_epoch_best_fitness = float('inf')\n",
    "current_epoch_best_particle_idx = -1\n",
    "for i, particle in enumerate(particles):\n",
    "    print(f\"  Evaluating initial fitness for particle {i+1}/{args.num_particles}:\")\n",
    "    # Use the NEW fitness function\n",
    "    fitness = evaluate_fitness_loss_and_grad_norm(\n",
    "        999, 999, args.lambda_gnp\n",
    "    )\n",
    "    particle['current_fitness'] = fitness\n",
    "    particle['pbest_fitness'] = fitness # Initial pbest fitness\n",
    "\n",
    "    if fitness < current_epoch_best_fitness:\n",
    "        current_epoch_best_fitness = fitness\n",
    "        current_epoch_best_particle_idx = i\n",
    "\n",
    "# # Update global best (gbest) based on the initial evaluation\n",
    "# if current_epoch_best_particle_idx != -1 :\n",
    "#      initial_best_particle = particles[current_epoch_best_particle_idx]\n",
    "#      print(f\"\\nInitial Global Best Fitness (particle {current_epoch_best_particle_idx+1}): {current_epoch_best_fitness:.4f}\")\n",
    "#      gbest_fitness = current_epoch_best_fitness\n",
    "#      gbest_state_dict = copy.deepcopy(initial_best_particle['pbest_state_dict'])\n",
    "# else:\n",
    "#      print(\"\\nWarning: No valid fitness found in initial evaluation.\")\n",
    "#      # Fallback: use the originally loaded model as gbest\n",
    "#      gbest_fitness = evaluate_fitness_loss_and_grad_norm(avg_loss, avg_grad_norm, args.lambda_gnp)\n",
    "#      gbest_state_dict = copy.deepcopy(initial_model.state_dict())\n",
    "#      print(f\"Using loaded model as initial gbest (Fitness: {gbest_fitness:.4f})\")\n",
    "\n",
    "\n",
    "# -------------------- CNOGNP Main Loop --------------------\n",
    "print(f\"\\n==> Starting CNOGNP Fine-tuning for {args.cnognp_epochs} epochs...\")\n",
    "cnognp_start_time = time.time()\n",
    "\n",
    "# Use args.cnognp_epochs here\n",
    "for cnognp_epoch in range(args.cnognp_epochs):\n",
    "    print(f\"\\n--- CNOGNP Epoch {cnognp_epoch + 1}/{args.cnognp_epochs} ---\")\n",
    "    epoch_start_time = time.time()\n",
    "    current_epoch_best_fitness = float('inf')\n",
    "    current_epoch_best_particle_idx = -1\n",
    "\n",
    "    for i, particle in enumerate(particles):\n",
    "        print(f\"  Processing Particle {i+1}/{args.num_particles}:\")\n",
    "        z_i_state = copy.deepcopy(particle['model'].state_dict())\n",
    "\n",
    "\n",
    "\n",
    "        # --- CNOGNP Line 7: Perform SGD step ---\n",
    "        z_bar_i_state, sgd_run_loss, avg_grad_norm = train_one_sgd_epoch_and_get_state(\n",
    "            z_i_state, trainloader, criterion, device,\n",
    "            args.eta, args.inner_sgd_momentum, args.inner_sgd_wd, args.model\n",
    "        )\n",
    "        new_state = copy.deepcopy(z_bar_i_state)\n",
    "        particle['model'].load_state_dict(new_state)\n",
    "        current_fitness = evaluate_fitness_loss_and_grad_norm(\n",
    "            sgd_run_loss, avg_grad_norm, args.lambda_gnp\n",
    "        )\n",
    "        particle['current_fitness'] = current_fitness\n",
    "\n",
    "        # --- CNOGNP Lines 11-13: Update PBest ---\n",
    "        if current_fitness < particle['pbest_fitness']:\n",
    "            print(f\"      New pbest for particle {i+1}: {current_fitness:.4f} (was {particle['pbest_fitness']:.4f})\")\n",
    "            particle['pbest_fitness'] = current_fitness\n",
    "            particle['pbest_state_dict'] = copy.deepcopy(particle['model'].state_dict())\n",
    "        else:\n",
    "            print(f\"      Fitness {current_fitness:.4f} not better than pbest {particle['pbest_fitness']:.4f}\")\n",
    "\n",
    "        if current_fitness < current_epoch_best_fitness:\n",
    "             current_epoch_best_fitness = current_fitness\n",
    "             current_epoch_best_particle_idx = i\n",
    "\n",
    "        # --- CNOGNP Lines 14-16: Update GBest ---\n",
    "        print(\"  Updating gbest...\")\n",
    "        if current_epoch_best_particle_idx != -1 and current_epoch_best_fitness < gbest_fitness:\n",
    "            print(f\"    New Global Best! Fitness: {current_epoch_best_fitness:.4f} (was {gbest_fitness:.4f}) from particle {current_epoch_best_particle_idx+1}'s pbest\")\n",
    "            gbest_fitness = current_epoch_best_fitness\n",
    "            gbest_state_dict = copy.deepcopy(particles[current_epoch_best_particle_idx]['pbest_state_dict'])\n",
    "        else:\n",
    "            print(f\"    No new gbest found this epoch. Best this epoch: {current_epoch_best_fitness:.4f}, Current gbest: {gbest_fitness:.4f}\")\n",
    "\n",
    "        # --- CNOGNP Line 8: Update Velocity ---\n",
    "        update_cno_velocity(\n",
    "            particle['velocity'], z_bar_i_state,\n",
    "            particle['pbest_state_dict'], gbest_state_dict,\n",
    "            args.c1, args.c2, device\n",
    "        )\n",
    "\n",
    "        # --- CNOGNP Line 9: Update Position ---\n",
    "        update_particle_position(\n",
    "             particle['model'], z_bar_i_state, particle['velocity'], device\n",
    "        )\n",
    "\n",
    "        # --- CNOGNP Line 10: Evaluate Fitness (Loss + λ*||g||₂) ---\n",
    "        # Use the NEW fitness function\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"--- CNOGNP Epoch {cnognp_epoch + 1} finished. Time: {epoch_time:.2f}s ---\")\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "total_cnognp_time = time.time() - cnognp_start_time\n",
    "print(f\"\\n==> Finished CNOGNP Fine-tuning in {total_cnognp_time:.2f} seconds ({total_cnognp_time/3600:.2f} hours).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Evaluating the best model found by CNOGNP...\n",
      "--- Final Training Set Evaluation (using standard evaluate) ---\n",
      "Train Eval | Loss: 0.0024 | Acc: 99.992% (49996/50000)\n",
      "--- Final Test Set Evaluation (using standard evaluate) ---\n",
      "Test  Eval | Loss: 0.2423 | Acc: 93.980% (9398/10000)\n",
      "\n",
      "--- Evaluating Final State of Particle 1/2 ---\n",
      "Particle 1 Train Set Evaluation:\n",
      "P1 Train Eval | Loss: 0.0025 | Acc: 99.982% (49991/50000)\n",
      "Particle 1 Test  Set Evaluation:\n",
      "P1 Test  Eval | Loss: 0.2448 | Acc: 93.950% (9395/10000)\n",
      "\n",
      "--- Evaluating Final State of Particle 2/2 ---\n",
      "Particle 2 Train Set Evaluation:\n",
      "P2 Train Eval | Loss: 0.0024 | Acc: 99.994% (49997/50000)\n",
      "Particle 2 Test  Set Evaluation:\n",
      "P2 Test  Eval | Loss: 0.2435 | Acc: 93.940% (9394/10000)\n",
      "\n",
      "===== Initial Model Performance =====\n",
      "Initial Training Loss: 0.0025\n",
      "Initial Training Acc:  99.988%\n",
      "Initial Test Loss:     0.2423\n",
      "Initial Test Acc:      94.020%\n",
      "====================================\n",
      "\n",
      "===== CNOGNP Fine-tuned Model Performance =====\n",
      "Achieved Global Best Fitness (Min Loss+λ||g||₂ during CNOGNP): 0.0450\n",
      "Final Eval Training Loss: 0.0024\n",
      "Final Eval Training Acc:  99.992%\n",
      "Final Eval Test Loss:     0.2423\n",
      "Final Eval Test Acc:      93.980%\n",
      "===========================================\n",
      "==> Saving final CNOGNP best model to ./resnet110_cifar10_cnognp_ft.pth\n",
      "Final best model saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Final Evaluation --------------------\n",
    "print(\"\\n==> Evaluating the best model found by CNOGNP...\")\n",
    "# final_best_model = ResNet20().to(device)\n",
    "if args.model == 'r20':\n",
    "    final_best_model = ResNet20().to(device)\n",
    "elif args.model == 'r32':\n",
    "    final_best_model = ResNet32().to(device)\n",
    "elif args.model == 'r44':\n",
    "    final_best_model = ResNet44().to(device)\n",
    "elif args.model == 'r56':\n",
    "    final_best_model = ResNet56().to(device)\n",
    "elif args.model == 'r110':\n",
    "    final_best_model = ResNet110().to(device)\n",
    "elif args.model == 'vit-t':\n",
    "    final_best_model = VisionTransformer(img_size=32,patch_size=4,num_classes=100, embed_dim=192, depth=12, num_heads=3).to(device)\n",
    "elif args.model == 'vit-s':\n",
    "    final_best_model = VisionTransformer(img_size=32,patch_size=4,num_classes=100, embed_dim=384, depth=12, num_heads=6).to(device)\n",
    "elif args.model == 'vgg16':\n",
    "    final_best_model = VGG('VGG16').to(device)\n",
    "elif args.model == 'vgg11':\n",
    "    final_best_model = VGG('VGG11').to(device)\n",
    "\n",
    "\n",
    "if gbest_state_dict is not None:\n",
    "    final_best_model.load_state_dict(gbest_state_dict)\n",
    "else:\n",
    "    print(\"Error: Global best state dictionary was not set. Cannot evaluate.\")\n",
    "    exit()\n",
    "\n",
    "# Use standard evaluate for final Loss/Acc comparison\n",
    "print(\"--- Final Training Set Evaluation (using standard evaluate) ---\")\n",
    "final_train_loss, final_train_acc = evaluate(trainloader, final_best_model, \"Train\")\n",
    "print(\"--- Final Test Set Evaluation (using standard evaluate) ---\")\n",
    "final_test_loss, final_test_acc = evaluate(testloader, final_best_model, \"Test\")\n",
    "particle_eval_results = {} # Optional: dictionary to store results per particle\n",
    "for i, particle in enumerate(particles):\n",
    "    print(f\"\\n--- Evaluating Final State of Particle {i+1}/{args.num_particles} ---\")\n",
    "    # The model in particle['model'] holds the final state after all updates\n",
    "    particle_model = particle['model']\n",
    "\n",
    "    # Use standard evaluate for training set\n",
    "    print(f\"Particle {i+1} Train Set Evaluation:\")\n",
    "    train_loss, train_acc = evaluate(trainloader, particle_model, f\"P{i+1} Train\")\n",
    "\n",
    "    # Use standard evaluate for test set\n",
    "    print(f\"Particle {i+1} Test  Set Evaluation:\") # Added padding for alignment\n",
    "    test_loss, test_acc = evaluate(testloader, particle_model, f\"P{i+1} Test \")\n",
    "\n",
    "    particle_eval_results[f'particle_{i+1}'] = {'train_loss': train_loss, 'train_acc': train_acc, 'test_loss': test_loss, 'test_acc': test_acc}\n",
    "\n",
    "print(\"\\n===== Initial Model Performance =====\")\n",
    "print(f\"Initial Training Loss: {initial_train_loss:.4f}\")\n",
    "print(f\"Initial Training Acc:  {initial_train_acc:.3f}%\")\n",
    "print(f\"Initial Test Loss:     {initial_test_loss:.4f}\")\n",
    "print(f\"Initial Test Acc:      {initial_test_acc:.3f}%\")\n",
    "print(\"====================================\")\n",
    "\n",
    "print(\"\\n===== CNOGNP Fine-tuned Model Performance =====\")\n",
    "print(f\"Achieved Global Best Fitness (Min Loss+λ||g||₂ during CNOGNP): {gbest_fitness:.4f}\")\n",
    "print(f\"Final Eval Training Loss: {final_train_loss:.4f}\") # Standard Loss\n",
    "print(f\"Final Eval Training Acc:  {final_train_acc:.3f}%\")\n",
    "print(f\"Final Eval Test Loss:     {final_test_loss:.4f}\") # Standard Loss\n",
    "print(f\"Final Eval Test Acc:      {final_test_acc:.3f}%\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# -------------------- Save Final Model --------------------\n",
    "print(f'==> Saving final CNOGNP best model to {args.save_path}')\n",
    "save_dir = os.path.dirname(args.save_path)\n",
    "if save_dir and not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "if gbest_state_dict is not None:\n",
    "    torch.save(gbest_state_dict, args.save_path)\n",
    "    print(\"Final best model saved.\")\n",
    "else:\n",
    "    print(\"Error: Global best state dictionary was not set. Model not saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sulw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
