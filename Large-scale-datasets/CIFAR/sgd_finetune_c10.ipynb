{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building and loading pre-trained model..\n",
      "Loading checkpoint from './resnet110_cifar10_final.pth'\n",
      "Pre-trained model loaded successfully.\n",
      "\n",
      "==> Evaluating loaded model before fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1118339/1081604056.py:222: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.load_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | Loss: 0.0025 | Acc: 99.988% (49994/50000) | Time: 11.47s\n",
      "Test  | Loss: 0.2423 | Acc: 94.020% (9402/10000) | Time: 2.77s\n",
      "--------------------------------------------------\n",
      "==> Starting SGD Fine-tuning...\n",
      "\n",
      "--- SGD Fine-tuning Epoch: 1/10 ---\n",
      "Train | Loss: 0.0047 | Acc: 99.942% (49971/50000) | LR: 0.00100 | Time: 27.89s\n",
      "Test  | Loss: 0.2448 | Acc: 93.990% (9399/10000) | Time: 2.58s\n",
      "\n",
      "--- SGD Fine-tuning Epoch: 2/10 ---\n",
      "Train | Loss: 0.0043 | Acc: 99.944% (49972/50000) | LR: 0.00100 | Time: 28.83s\n",
      "Test  | Loss: 0.2437 | Acc: 93.970% (9397/10000) | Time: 2.55s\n",
      "\n",
      "--- SGD Fine-tuning Epoch: 3/10 ---\n",
      "Train | Loss: 0.0042 | Acc: 99.952% (49976/50000) | LR: 0.00100 | Time: 27.06s\n",
      "Test  | Loss: 0.2427 | Acc: 93.880% (9388/10000) | Time: 2.45s\n",
      "\n",
      "--- SGD Fine-tuning Epoch: 4/10 ---\n",
      "Train | Loss: 0.0041 | Acc: 99.960% (49980/50000) | LR: 0.00100 | Time: 27.55s\n",
      "Test  | Loss: 0.2423 | Acc: 93.960% (9396/10000) | Time: 2.50s\n",
      "\n",
      "--- SGD Fine-tuning Epoch: 5/10 ---\n",
      "Train | Loss: 0.0044 | Acc: 99.962% (49981/50000) | LR: 0.00100 | Time: 27.88s\n",
      "Test  | Loss: 0.2412 | Acc: 94.020% (9402/10000) | Time: 2.50s\n",
      "\n",
      "--- SGD Fine-tuning Epoch: 6/10 ---\n",
      "Train | Loss: 0.0041 | Acc: 99.970% (49985/50000) | LR: 0.00100 | Time: 27.64s\n",
      "Test  | Loss: 0.2434 | Acc: 93.990% (9399/10000) | Time: 2.80s\n",
      "\n",
      "--- SGD Fine-tuning Epoch: 7/10 ---\n",
      "Train | Loss: 0.0040 | Acc: 99.966% (49983/50000) | LR: 0.00100 | Time: 28.39s\n",
      "Test  | Loss: 0.2414 | Acc: 93.990% (9399/10000) | Time: 2.46s\n",
      "\n",
      "--- SGD Fine-tuning Epoch: 8/10 ---\n",
      "Train | Loss: 0.0044 | Acc: 99.952% (49976/50000) | LR: 0.00100 | Time: 27.81s\n",
      "Test  | Loss: 0.2426 | Acc: 93.970% (9397/10000) | Time: 2.52s\n",
      "\n",
      "--- SGD Fine-tuning Epoch: 9/10 ---\n",
      "Train | Loss: 0.0045 | Acc: 99.962% (49981/50000) | LR: 0.00100 | Time: 28.26s\n",
      "Test  | Loss: 0.2442 | Acc: 94.000% (9400/10000) | Time: 2.51s\n",
      "\n",
      "--- SGD Fine-tuning Epoch: 10/10 ---\n",
      "Train | Loss: 0.0044 | Acc: 99.946% (49973/50000) | LR: 0.00100 | Time: 27.15s\n",
      "Test  | Loss: 0.2445 | Acc: 93.910% (9391/10000) | Time: 2.55s\n",
      "\n",
      "==> Finished SGD Fine-tuning in 303.96 seconds.\n",
      "==> Saving final fine-tuned model to ./resnet110_cifar10_sgd_ft.pth\n",
      "Final fine-tuned model saved.\n",
      "\n",
      "==> Evaluating final fine-tuned model (after 10 epochs)...\n",
      "--- Final Training Set Evaluation ---\n",
      "Train | Loss: 0.0025 | Acc: 99.988% (49994/50000) | Time: 11.05s\n",
      "--- Final Test Set Evaluation ---\n",
      "Test  | Loss: 0.2445 | Acc: 93.910% (9391/10000) | Time: 2.49s\n",
      "\n",
      "===== Initial Model Performance =====\n",
      "Initial Training Loss: 0.0025\n",
      "Initial Training Acc:  99.988%\n",
      "Initial Test Loss:     0.2423\n",
      "Initial Test Acc:      94.020%\n",
      "====================================\n",
      "\n",
      "===== Final Fine-tuned Model Performance =====\n",
      "Final Training Loss: 0.0025\n",
      "Final Training Acc:  99.988%\n",
      "Final Test Loss:     0.2445\n",
      "Final Test Acc:      93.910%\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, num=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# -------------------- ResNet20 定义 --------------------\n",
    "# (ResNet20 定义代码保持不变，此处省略以节省空间)\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "        # Don't apply weight init here if loading pretrained weights\n",
    "        # self.apply(_weights_init)\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride_val in strides: # Renamed variable to avoid conflict\n",
    "            layers.append(block(self.in_planes, planes, stride_val)) # Use renamed variable\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # Use adaptive average pooling for robustness\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3], num_classes=10)\n",
    "def ResNet32():\n",
    "    \"\"\" ResNet-32 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (32-2)/6 = 5\n",
    "    return ResNet(BasicBlock, [5, 5, 5], num_classes=10)\n",
    "\n",
    "def ResNet44():\n",
    "    \"\"\" ResNet-44 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (44-2)/6 = 7\n",
    "    return ResNet(BasicBlock, [7, 7, 7], num_classes=10)\n",
    "\n",
    "def ResNet56():\n",
    "    \"\"\" ResNet-56 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (56-2)/6 = 9\n",
    "    return ResNet(BasicBlock, [9, 9, 9], num_classes=10)\n",
    "\n",
    "def ResNet110():\n",
    "    \"\"\" ResNet-110 model configuration based on 6n+2 formula \"\"\"\n",
    "    # (110-2)/6 = 18\n",
    "    return ResNet(BasicBlock, [18, 18, 18], num_classes=10)\n",
    "\n",
    "# Hardcode the values instead:\n",
    "class Args:\n",
    "    model = 'r110'\n",
    "    load_path = './resnet110_cifar10_final.pth'\n",
    "    ft_epochs = 10\n",
    "    ft_lr = 0.001  # 0.001\n",
    "    save_path = './resnet110_cifar10_sgd_ft.pth'\n",
    "    batch_size = 128\n",
    "    data_path = './data'\n",
    "args = Args()\n",
    "\n",
    "\n",
    "# -------------------- 设备配置 --------------------\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------- 数据准备 --------------------\n",
    "print('==> Preparing data..')\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=args.data_path, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=args.data_path, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# -------------------- 模型加载 --------------------\n",
    "print('==> Building and loading pre-trained model..')\n",
    "# net = ResNet20()\n",
    "if args.model == 'r20':\n",
    "    net = ResNet20().to(device)\n",
    "elif args.model == 'r32':\n",
    "    net = ResNet32().to(device)\n",
    "elif args.model == 'r44':\n",
    "    net = ResNet44().to(device)\n",
    "elif args.model == 'r56':\n",
    "    net = ResNet56().to(device)\n",
    "elif args.model == 'r110':\n",
    "    net = ResNet110().to(device)\n",
    "elif args.model == 'vgg16':\n",
    "    net = VGG('VGG16').to(device)\n",
    "elif args.model == 'vgg11':\n",
    "    net = VGG('VGG11').to(device)\n",
    "elif args.model == 'vit-t':\n",
    "    net = VisionTransformer(img_size=32,patch_size=4,num_classes=100, embed_dim=192, depth=12, num_heads=3).to(device)\n",
    "elif args.model == 'vit-s':\n",
    "    net = VisionTransformer(img_size=32,patch_size=4,num_classes=100, embed_dim=384, depth=12, num_heads=6).to(device)\n",
    "\n",
    "\n",
    "if os.path.exists(args.load_path):\n",
    "    try:\n",
    "        print(f\"Loading checkpoint from '{args.load_path}'\")\n",
    "        checkpoint = torch.load(args.load_path, map_location=device)\n",
    "        # Adjust based on how the model was saved (state_dict vs full model)\n",
    "        if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
    "             net.load_state_dict(checkpoint['state_dict'])\n",
    "        elif isinstance(checkpoint, dict) and not ('state_dict' in checkpoint): # Directly saved state_dict\n",
    "             net.load_state_dict(checkpoint)\n",
    "        else: # Saved the entire model object\n",
    "             net = checkpoint\n",
    "        print(\"Pre-trained model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        print(\"Proceeding with initialized ResNet20 (training from scratch).\")\n",
    "        # Apply weight initialization if not loading weights\n",
    "        net.apply(_weights_init)\n",
    "else:\n",
    "    print(f\"Checkpoint file not found at '{args.load_path}'.\")\n",
    "    print(\"Proceeding with initialized ResNet20 (training from scratch).\")\n",
    "    # Apply weight initialization if not loading weights\n",
    "    net.apply(_weights_init)\n",
    "\n",
    "\n",
    "# Optional DataParallel\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "#     net = torch.nn.DataParallel(net)\n",
    "#     cudnn.benchmark = True # Good if input sizes don't change\n",
    "\n",
    "# -------------------- 损失函数 和 sgd 优化器 --------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# --- 添加 SGD 优化器定义 ---\n",
    "optimizer = optim.SGD(net.parameters(),       # 使用模型的参数\n",
    "                      lr=args.ft_lr,          # 使用 fine-tuning 的学习率\n",
    "                      momentum=0,           # 动量\n",
    "                      weight_decay=5e-4)      # 权重衰减 (如果需要的话)\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "# No learning rate scheduler needed for fixed LR fine-tuning\n",
    "\n",
    "# -------------------- 评估函数 (Same as before) --------------------\n",
    "def evaluate(loader, set_name=\"Test\", model=net): # Pass model explicitly\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs) # Use the passed model\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    avg_loss = eval_loss / len(loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'{set_name.ljust(5)} | Loss: {avg_loss:.4f} | Acc: {accuracy:.3f}% ({correct}/{total}) | Time: {epoch_time:.2f}s')\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# -------------------- 初始评估 (评估加载的模型) --------------------\n",
    "print(\"\\n==> Evaluating loaded model before fine-tuning...\")\n",
    "initial_train_loss, initial_train_acc = evaluate(trainloader, \"Train\", net)\n",
    "initial_test_loss, initial_test_acc = evaluate(testloader, \"Test\", net)\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "\n",
    "def train_sgd(epoch): # <--- 修改函数名 (或者你喜欢的其他名字)\n",
    "    print(f'\\n--- SGD Fine-tuning Epoch: {epoch+1}/{args.ft_epochs} ---') # 更新打印信息\n",
    "    net.train() # Set model to training mode\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    current_lr = optimizer.param_groups[0]['lr'] # Get LR (should be fixed)\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # --- 标准 SGD 步骤 ---\n",
    "        optimizer.zero_grad()       # <--- 添加：梯度清零\n",
    "        outputs = net(inputs)       # <--- 前向传播 (只需要一次)\n",
    "        loss = criterion(outputs, targets) # <--- 计算损失\n",
    "        loss.backward()             # <--- 反向传播 (只需要一次)\n",
    "        optimizer.step()            # <--- 添加：更新权重\n",
    "        # --- 结束标准 SGD 步骤 ---\n",
    "\n",
    "\n",
    "        # --- 删除 SAM 相关步骤 ---\n",
    "        # # 1. First forward/backward pass ...\n",
    "        # loss.backward()\n",
    "        # optimizer.first_step(zero_grad=True) # <-- 删除\n",
    "\n",
    "        # # 2. Second forward/backward pass ...\n",
    "        # with torch.enable_grad():\n",
    "        #      criterion(net(inputs), targets).backward() # <-- 删除\n",
    "        # optimizer.second_step(zero_grad=True) # <-- 删除\n",
    "        # --- 结束删除 SAM 步骤 ---\n",
    "\n",
    "\n",
    "        # (损失和准确率的累积部分保持不变)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1) # Use predictions from the forward pass\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # (打印 epoch 结果的部分保持不变)\n",
    "    epoch_time = time.time() - start_time\n",
    "    epoch_loss = train_loss / len(trainloader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    print(f'Train | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.3f}% ({correct}/{total}) | LR: {current_lr:.5f} | Time: {epoch_time:.2f}s')\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# --- 修改 \"Fine-tuning 主循环\" ---\n",
    "print(\"==> Starting SGD Fine-tuning...\") # 更新打印信息\n",
    "finetuning_start_time = time.time()\n",
    "\n",
    "for epoch in range(args.ft_epochs):\n",
    "    # 1. Fine-tune with SGD for one epoch\n",
    "    # train_loss, train_acc = train_sam(epoch) # 原来的调用\n",
    "    train_loss, train_acc = train_sgd(epoch) # <--- 使用新的函数名\n",
    "\n",
    "    # (评估部分保持不变)\n",
    "    test_loss, test_acc = evaluate(testloader, \"Test\", net)\n",
    "\n",
    "total_finetuning_time = time.time() - finetuning_start_time\n",
    "print(f\"\\n==> Finished SGD Fine-tuning in {total_finetuning_time:.2f} seconds.\") # 更新打印信息\n",
    "\n",
    "\n",
    "# -------------------- 保存最终 Fine-tuned 模型 --------------------\n",
    "print(f'==> Saving final fine-tuned model to {args.save_path}')\n",
    "save_dir = os.path.dirname(args.save_path)\n",
    "if save_dir and not os.path.exists(save_dir): # Check if save_dir is not empty\n",
    "    os.makedirs(save_dir)\n",
    "# Save only the model state_dict is usually preferred\n",
    "torch.save(net.state_dict(), args.save_path)\n",
    "# If you need to save optimizer state as well (e.g., to resume SAM training):\n",
    "# torch.save({\n",
    "#     'epoch': args.ft_epochs, # Or the actual last epoch number\n",
    "#     'model_state_dict': net.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(), # Save SAM state\n",
    "# }, args.save_path)\n",
    "print(\"Final fine-tuned model saved.\")\n",
    "\n",
    "# -------------------- 评估最终 Fine-tuned 模型 --------------------\n",
    "print(\"\\n==> Evaluating final fine-tuned model (after {} epochs)...\".format(args.ft_epochs))\n",
    "print(\"--- Final Training Set Evaluation ---\")\n",
    "final_train_loss, final_train_acc = evaluate(trainloader, \"Train\", net)\n",
    "print(\"--- Final Test Set Evaluation ---\")\n",
    "final_test_loss, final_test_acc = evaluate(testloader, \"Test\", net)\n",
    "\n",
    "print(\"\\n===== Initial Model Performance =====\")\n",
    "print(f\"Initial Training Loss: {initial_train_loss:.4f}\")\n",
    "print(f\"Initial Training Acc:  {initial_train_acc:.3f}%\")\n",
    "print(f\"Initial Test Loss:     {initial_test_loss:.4f}\")\n",
    "print(f\"Initial Test Acc:      {initial_test_acc:.3f}%\")\n",
    "print(\"====================================\")\n",
    "\n",
    "print(\"\\n===== Final Fine-tuned Model Performance =====\")\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final Training Acc:  {final_train_acc:.3f}%\")\n",
    "print(f\"Final Test Loss:     {final_test_loss:.4f}\")\n",
    "print(f\"Final Test Acc:      {final_test_acc:.3f}%\")\n",
    "print(\"==========================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sulw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
